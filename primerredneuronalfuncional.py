# -*- coding: utf-8 -*-
"""Copia de Noriega Rocha Bryan Axel_PRIMERREDNEURONALFUNCIONAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yxCS0oA7GSLzRhGehkPZUxJWvTzxTS4a
"""

import torch  # this is the baseline functionality of Pytorch
import matplotlib.pyplot as plt

import pandas as pd

from torchvision import datasets # a module that let's you load popular datasets
                                 # for training a neural network

import torchvision.transforms as transforms  # The data in datasets is not
                                  # always in the format we need. This module
                                  #let's you easily modify

import torchvision.utils as vision_utils  # submodule with useful functions for
                                  # handling images

import torch.optim as optim  # submodule wiht different otimization (or learning)
                            # algorithms

# Each sample in the dataset is a tuple (image:PIL[1,28,28],label:int)
train_ds = datasets.MNIST('../data', train=True, download=True)
test_ds = datasets.MNIST('../data', train=False, download=True)

print(train_ds)
print(type(train_ds))

print('The number of images in the set is:')
print(len(train_ds))

print('The type of the first element in train_ds is:')
print(type(train_ds[0]))

print('The number of elements in the first tuple of train_ds is:')
print(len(train_ds[0]))

print('\nThe first element in the first tuple of train_ds is of type:')  # "\n" prints a space before
print(type(train_ds[0][0]))

print('\nThe second element in the first tuple of train_ds is of type:') # "\n" prints a space before
print(type(train_ds[0][1]))

transform = transforms.ToTensor()

train_ds.transform =transform
test_ds.transform = transform

print('The new type of the first image in train_ds is')
print(type(train_ds[0][0]))

print('The type of the first image in the train dataset is: ')
print(type(train_ds[0][0]))  # first tuple of the dataset, first element of the tuple

print('\nThe shape attribute of the image Tensor of the train dataset is: ')
print(train_ds[0][0].shape)

image0 = train_ds[0][0]  # first tuple in the dataset, first element in the tuple
print(type(image0))
print(image0.shape)

print('The first row of the image is:')
print(image0[0,0,0:28])  # first channel, first row, columns 0 to 27 (because
                         # the last index is not included in the range construct
                         # begin:end)

print('The first column of the image is:')
print(image0[0,0:28,0])  # first channel, rows 0 to 27, first column

import torch.utils as utils  # note that you can import anywhere in your
                                      # code as long as you do it before you use
                                      # the module

batch_size=32  # numer of elements to retrieve at once from the dataset

train_loader = utils.data.DataLoader(train_ds, # dataset to get the data from
                                    batch_size=batch_size,
                                    shuffle=True)
# note that you can use multi-line statements in python

test_loader = utils.data.DataLoader(test_ds,
                                   batch_size=batch_size,
                                   shuffle=True)

"""So let's get a batch of images and labels from **train_loader** using its iterator:"""

print('The number of batches in the loader should be:')
print(60000/32)

print('and it actually is:')
print(len(train_loader))  # the loader class allows using python's len()

# get an image from the loader
loader_iterator = iter(train_loader)
images, labels = next(loader_iterator)

print('The shape of the images batch Tensor is')
print(images.shape)

n_batches = 10
C = 3
H = 28
W = 28
total_pxs = C*H*W
shape = (n_batches, C, H, W)  # shape is a python tuple

rand_image = torch.rand(shape)  # rand returns a tensor of the given shape

print(f'our random image is of shape: {rand_image.shape}')
print(f'the flattened image should be of shape [{n_batches}, {total_pxs}]')

a_flatten_layer = torch.nn.Flatten(start_dim=1)
flattened_image = a_flatten_layer(rand_image)
print(f'our flattened image is of shape: {flattened_image.shape}')

N = 15  # number of neurons in the layer. Also, the number of outputs of the layer
a_linear_layer = torch.nn.Linear(total_pxs,N)
output = a_linear_layer(flattened_image)

print('The shape of the output tensor is:')
print(output.shape)

print('The shape of the weights tensor is [N, total_pxs]:')
print(a_linear_layer.weight.shape)

C = 1  # monochromatic images
H = 28  # 28 pixels per side
W = 28  # 28 pixels per side

n_inputs = C*H*W  # total number of inputs is the total number of pixels
N1 = 800  # number of neurons in layer 1, the input layer
n_outputs = 10  # number of neurons at layer 2, the output layer

# let's build our network using a Sequential container
net = torch.nn.Sequential(
            torch.nn.Flatten(start_dim=1),  # start at 2nd dim to preserve the batch dim
            torch.nn.Linear(n_inputs, N1),  # n_inputs connecting to N1 neurons
            torch.nn.Linear(N1, n_outputs), # N1 outputs from last layer are inputs to this layer with n_outputs neurons
        )

criterion = torch.nn.CrossEntropyLoss()

optimizer = optim.SGD(net.parameters(), lr=0.001)

red01=[]
n_epochs = 5  # number of epochs we want to train
for epoch in range(n_epochs):  # range(int_x) converts an integer into a list from 0 to int_x
    for batch_idx, data in enumerate(train_loader):  # consume the dataset, one batch at a time
        # get the inputs; data is a tuple of (inputs, labels)
        inputs, labels = data

        # zero the parameter gradients to get the gradient per batch
        optimizer.zero_grad()

        # do a forward pass
        outputs = net(inputs)

        # compute the loss
        loss = criterion(outputs, labels)

        # do the backpropagation
        loss.backward()  # backward is an attribute of every tensor.
                         # Thus, you can compute the gradient on any tensor that
                         # that results from any differentiable operation.

        # Let the parameters with the optimizer
        optimizer.step()

        # print statistics
        if batch_idx % 100 == 0:    # print every 100 mini-batches
          print(f'[{epoch}, {batch_idx:5d}] loss: {loss.item():.3f}')
        red01.append(loss.item())
        plt.plot(red01)
        plt.show
        model = torch.save(net, 'model1.pth')

preds_hist=[]
exp_hist=[]
MC=torch.zeros(10,10)
for batch_idx, data in enumerate(test_loader):  # consume the dataset, one batch at a time
        # get the inputs; data is a tuple of (inputs, labels)
        inputs, labels = data

        # do a forward pass
        outputs = net(inputs)

        preds=torch.argmax(outputs,dim=1)
        preds_hist.append(preds.detach())
        exp_hist.append(labels.detach())

for i in range(len(preds_hist)):
  for j in range(len(exp_hist[i])):
    MC[preds_hist[i][j], exp_hist[i][j]]+=1

MC
conf_matrix = MC
conf_df = pd.DataFrame(conf_matrix)
conf_df.to_csv("confusion_matrix1.csv", index=False)
MC

sum_MC = torch.sum(MC)
print("Suma de la matriz de confusion: ", sum_MC)
plt.imshow(MC, cmap=plt.cm.Blues)
plt.colorbar()
plt.xlabel('Expected')
plt.ylabel('Predicted')
plt.show()
plt.savefig("Modelo1.jpg")

"""# **2do Red Neuronal**"""

#2da RED NEURONAL A ENTRENAR
C1 = 1  # monochromatic images
H1 = 28  # 28 pixels per side
W1 = 28  # 28 pixels per side

n_inputs1 = C1*H1*W1  # total number of inputs is the total number of pixels
N11 = 1000  # number of neurons in layer 1, the input layer
n_outputs1 = 10  # number of neurons at layer 2, the output layer

# let's build our network using a Sequential container
net1 = torch.nn.Sequential(
            torch.nn.Flatten(start_dim=1),  # start at 2nd dim to preserve the batch dim
            torch.nn.Linear(n_inputs1, N11),  # n_inputs connecting to N1 neurons
            torch.nn.Linear(N11, n_outputs1), # N1 outputs from last layer are inputs to this layer with n_outputs neurons
        )

#Criterio de 2da RED NEURONAL
criterion2 = torch.nn.CrossEntropyLoss()

#Optimizador de 2da RED NEURONAL
optimizer1 = optim.SGD(net1.parameters(), lr=0.001)

#Entrenamiento 2da red neuronal
red02=[]
n_epochs2 = 5  # number of epochs we want to train
for epoch in range(n_epochs2):  # range(int_x) converts an integer into a list from 0 to int_x
    for batch_idx, data in enumerate(train_loader):  # consume the dataset, one batch at a time
        # get the inputs; data is a tuple of (inputs, labels)
        inputs, labels = data

        # zero the parameter gradients to get the gradient per batch
        optimizer1.zero_grad()

        # do a forward pass
        outputs = net1(inputs)

        # compute the loss
        loss = criterion2(outputs, labels)

        # do the backpropagation
        loss.backward()  # backward is an attribute of every tensor.
                         # Thus, you can compute the gradient on any tensor that
                         # that results from any differentiable operation.

        # Let the parameters with the optimizer
        optimizer1.step()

        # print statistics
        if batch_idx % 100 == 0:    # print every 100 mini-batches
          print(f'[{epoch}, {batch_idx:5d}] loss: {loss.item():.3f}')
        red02.append(loss.item())
        plt.plot(red02)
        plt.show
        model2 = torch.save(net1, 'model2.pth')

preds_hist1=[]
exp_hist1=[]
MC1=torch.zeros(10,10)
for batch_idx, data in enumerate(test_loader):  # consume the dataset, one batch at a time
        # get the inputs; data is a tuple of (inputs, labels)
        inputs, labels = data

        # do a forward pass
        outputs = net1(inputs)

        preds=torch.argmax(outputs,dim=1)
        preds_hist1.append(preds.detach())
        exp_hist1.append(labels.detach())

for i in range(len(preds_hist1)):
  for j in range(len(exp_hist1[i])):
    MC1[preds_hist1[i][j], exp_hist1[i][j]]+=1
#sum_MC1=torch.sum(MC1)
#print ("suma",sum_MC1)
MC1
conf_matrix2 = MC1
conf_df2 = pd.DataFrame(conf_matrix2)
conf_df2.to_csv("confusion_matrix2.csv", index=False)
MC1

sum_MC1 = torch.sum(MC1)
print("Suma de la matriz de confusion: ", sum_MC1)
plt.imshow(MC1, cmap=plt.cm.Blues)
plt.colorbar()
plt.xlabel('Expected')
plt.ylabel('Predicted')
plt.show()
plt.savefig("Modelo2.jpg")

"""# **3ra Red Neuronal**"""

#3ra RED NEURONAL A ENTRENAR
C2 = 1  # monochromatic images
H2 = 28  # 28 pixels per side
W2 = 28  # 28 pixels per side

n_inputs2 = C2*H2*W2  # total number of inputs is the total number of pixels
N12 = 700  # number of neurons in layer 1, the input layer
n_outputs2 = 10  # number of neurons at layer 2, the output layer

# let's build our network using a Sequential container
net2 = torch.nn.Sequential(
            torch.nn.Flatten(start_dim=1),  # start at 2nd dim to preserve the batch dim
            torch.nn.Linear(n_inputs2, N12),  # n_inputs connecting to N1 neurons
            torch.nn.Linear(N12, n_outputs2), # N1 outputs from last layer are inputs to this layer with n_outputs neurons
        )

#Criterio 3ra RED NEURONAL
criterion3 = torch.nn.CrossEntropyLoss()

#Optimizador de 3ra RED NEURONAL
optimizer3 = optim.SGD(net2.parameters(), lr=0.001)

#Entrenamiento 3ra red neuronal
red03=[]
n_epochs3 = 5  # number of epochs we want to train
for epoch in range(n_epochs3):  # range(int_x) converts an integer into a list from 0 to int_x
    for batch_idx, data in enumerate(train_loader):  # consume the dataset, one batch at a time
        # get the inputs; data is a tuple of (inputs, labels)
        inputs, labels = data

        # zero the parameter gradients to get the gradient per batch
        optimizer3.zero_grad()

        # do a forward pass
        outputs = net2(inputs)

        # compute the loss
        loss = criterion(outputs, labels)

        # do the backpropagation
        loss.backward()  # backward is an attribute of every tensor.
                         # Thus, you can compute the gradient on any tensor that
                         # that results from any differentiable operation.

        # Let the parameters with the optimizer
        optimizer3.step()

        # print statistics
        if batch_idx % 100 == 0:    # print every 100 mini-batches
          print(f'[{epoch}, {batch_idx:5d}] loss: {loss.item():.3f}')
        red03.append(loss.item())
        plt.plot(red03)
        plt.show
        model3 = torch.save(net2, 'model3.pth')

#TEST
preds_hist2=[]
exp_hist2=[]
MC2=torch.zeros(10,10)
for batch_idx, data in enumerate(test_loader):  # consume the dataset, one batch at a time
        # get the inputs; data is a tuple of (inputs, labels)
        inputs, labels = data

        # do a forward pass
        outputs = net2(inputs)

        preds=torch.argmax(outputs,dim=1)
        preds_hist2.append(preds.detach())
        exp_hist2.append(labels.detach())

for i in range(len(preds_hist2)):
  for j in range(len(exp_hist2[i])):
    MC2[preds_hist2[i][j], exp_hist2[i][j]]+=1
#sum_MC2=torch.sum(MC2)
#print ("suma",sum_MC2)
MC2
conf_matrix3 = MC2
conf_df3 = pd.DataFrame(conf_matrix3)
conf_df3.to_csv("confusion_matrix3.csv", index=False)
MC2

sum_MC2 = torch.sum(MC2)
print("Suma de la matriz de confusion: ", sum_MC2)
plt.imshow(MC2, cmap=plt.cm.Blues)
plt.colorbar()
plt.xlabel('Expected')
plt.ylabel('Predicted')
plt.show()
plt.savefig("Modelo3.jpg")
#model3 = torch.save(net2, 'model3.pth')

"""# **Red sin entrenamiento**"""

#RED NEURONAL SIN ENTRENAR
C3 = 1  # monochromatic images
H3 = 28  # 28 pixels per side
W3 = 28  # 28 pixels per side

n_inputs3 = C3*H3*W3  # total number of inputs is the total number of pixels
N13 = 600  # number of neurons in layer 1, the input layer
n_outputs3 = 10  # number of neurons at layer 2, the output layer

# let's build our network using a Sequential container
net3 = torch.nn.Sequential(
            torch.nn.Flatten(start_dim=1),  # start at 2nd dim to preserve the batch dim
            torch.nn.Linear(n_inputs3, N13),  # n_inputs connecting to N1 neurons
            torch.nn.Linear(N13, n_outputs3), # N1 outputs from last layer are inputs to this layer with n_outputs neurons
        )

#Criterio 4ta RED NEURONAL
criterion4 = torch.nn.CrossEntropyLoss()

#Optimizador de 3ra RED NEURONAL
optimizer4 = optim.SGD(net2.parameters(), lr=0.001)
#TEST
preds_hist3=[]
exp_hist3=[]
MC3=torch.zeros(10,10)
for batch_idx, data in enumerate(test_loader):  # consume the dataset, one batch at a time
        # get the inputs; data is a tuple of (inputs, labels)
        inputs, labels = data

        # do a forward pass
        outputs = net3(inputs)

        preds=torch.argmax(outputs,dim=1)
        preds_hist3.append(preds.detach())
        exp_hist3.append(labels.detach())

for i in range(len(preds_hist3)):
  for j in range(len(exp_hist3[i])):
    MC3[preds_hist3[i][j], exp_hist3[i][j]]+=1
#sum_MC2=torch.sum(MC2)
#print ("suma",sum_MC2)
MC3
sum_MC3 = torch.sum(MC3)
print("Suma de la matriz de confusion: ", sum_MC3)
plt.imshow(MC3, cmap=plt.cm.Blues)
plt.colorbar()
plt.xlabel('Expected')
plt.ylabel('Predicted')
plt.show()
plt.savefig("Modelo4.jpg")

"""# **Grafica de las 3 redes.**"""

#Graficar las 3 redes
window_size = 400
numbers_series = pd.Series(red01)

windows = numbers_series.rolling(window_size)
moving_averages = windows.mean()
moving_averages_list = moving_averages.tolist()

final_list = moving_averages_list[window_size - 1:]

numbers_series = pd.Series(red02)
windows = numbers_series.rolling(window_size)
moving_averages = windows.mean()
moving_averages_list = moving_averages.tolist()

final_list2 = moving_averages_list[window_size -1:]

numbers_series = pd.Series(red03)
windows = numbers_series.rolling(window_size)
moving_averages = windows.mean()
moving_averages_list = moving_averages.tolist()
final_list3 = moving_averages_list[window_size -1:]

plt.plot(final_list,label="Modelo 1")
plt.plot(final_list2,label="Modelo 2")
plt.plot(final_list3,label="Modelo 3")
plt.legend()
plt.show()